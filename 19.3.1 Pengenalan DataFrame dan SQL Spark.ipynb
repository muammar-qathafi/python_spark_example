{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membaca data file ke DataFrame dan mengolahnya\n",
    "Berikut adalah kode untuk membaca data ke dalam DataFrame Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SparkContext' from 'pyspark.sql' (C:\\Users\\User\\anaconda3\\lib\\site-packages\\pyspark\\sql\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22988\\2978643165.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#import module/package yang dibutuhkan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#membuat session, untuk mengakses semua fungsi spark dan DataFrame API\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'SparkContext' from 'pyspark.sql' (C:\\Users\\User\\anaconda3\\lib\\site-packages\\pyspark\\sql\\__init__.py)"
     ]
    }
   ],
   "source": [
    "#import module/package yang dibutuhkan\n",
    "from pyspark.sql import SparkSession, SparkContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#membuat session, untuk mengakses semua fungsi spark dan DataFrame API\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pengenalan DataFrame Spark\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#mendefinisikan skema dari data yang kita baca\n",
    "purchaseSchema = StructType([\n",
    "    StructField(\"Tanggal\", DateType(), True),\n",
    "    StructField(\"Jam\", StringType(), True),\n",
    "    StructField(\"Kota\", StringType(), True),\n",
    "    StructField(\"Item\", StringType(), True),\n",
    "    StructField(\"Total\", FloatType(), True),\n",
    "    StructField(\"Pembayaran\", StringType(), True),\n",
    "])    \n",
    "\n",
    "#membaca file csv menggunakan skema yang kita buat sebelumya, \n",
    "#dan dengan pemisah kolom \"tab\" (\\t)\n",
    "purchaseDataframe = spark.read.csv(\n",
    "    \"dataset/purchases.csv\", \n",
    "    header=True, schema=purchaseSchema, sep=\"\\t\")\n",
    "#menampilkan 3 baris DataFrame\n",
    "purchaseDataframe.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertama kita perlu mengimport module/package yang dibutuhkan, yaitu SparkSession dan semua sql data type. Untuk mengakses semua fungsi Spark dan DataFrame API, kita harus membuat SparkSession. Kemudian kita akan membuat skema untuk data kita. Skema ini mencakup susunan kolom dari data kita, jenis data per kolomnya, dan apakah diperbolehkan jenis data null (kosong) atau tidak. Kita mendefinisikan skema data menggunakan \"StructType\" seperti kode di atas. Adapun \"StructType\" sudah kita import dari \"from pyspark.sql.types import \\*\", dimana \"\\*\" berarti import semua yang tersedia, termasuk \"StructType\". Setelah itu kita dapat membaca data CSV kita ke DataFrame Spark. Default pembacaan file CSV adalah melakukan pemisahan kolom dengan pemisahnya adalah tanda koma \",\". Dan karena data kita dipisahkan menggunakan \"tab\", maka kita tambahkan parameter \"sep=\"\\t\"\", dan juga kita isi Header=True karena file kita memiliki header kolom. Kemudian, kenapa kita harus menggunakan skema? Karena ini akan memudahkan untuk proses selanjutnya. Dengan skema, kita dapat menentukan jenis data dari setiap kolom. Misalnya di kolom \"Total\", kita isikan tipe data float, sehingga nanti kita dapat memfilter DataFrame ini, misalnya hanya menampilkan nilai di atas batas nilai tertentu. Terakhir, kita tampilkan tiga baris DataFrame dengan perintah \".show(3)\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya kita dapat menghitung jumlah baris data kita dengan perintah \".count()\" seperti kode di bawah ini. Kita juga dapat melihat skema kita dengan perintah \"printSchema()\". Untuk melihat statistik dari DataFrame, kita dapat melakukannya dengan perintah \".describe()\". Kita dapat menentukan kolom mana yang ingin kita lihat, di bawah ini misanya kolom \"Total\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#menghitung baris dari DataFrame kita, dan menge-print untuk menampilkan\n",
    "jumlahBaris = purchaseDataframe.count()\n",
    "print(\"jumlah baris: \", jumlahBaris)\n",
    "#melihat skema dari dataframe kita\n",
    "purchaseDataframe.printSchema()\n",
    "purchaseDataframe.describe('Total').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita dapat membuat DataFrame baru dari kolom data yang kita inginkan saja, yakni dengan menggunakan perintah \".select(nama_DataFrame['nama_kolom'])\", seperti kode di bawah ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#membuat DataFrame baru dengan mengambil data kolom kota \n",
    "#dan total pembayaran saja\n",
    "kotaTotalDataframe = purchaseDataframe.select(purchaseDataframe['Kota'], \n",
    "                                              purchaseDataframe['Total'])\n",
    "kotaTotalDataframe.show(3); #menampilkan 3 baris DataFrame baru kita\n",
    "kotaTotalDataframe.printSchema() #print skema dari DataFrame baru kita"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita dapat menambahkan konstanta nilai tertentu pada kolom yang kita inginkan, seperti kode di bawah ini. Coba lihat nilai setelah kita munculkan tabelnya, maka semua nilai di kolom \"Total\" akan ditambah dengan 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#menambahkan nilai 10 untuk nilai di kolom 'Total' DataFrame kita\n",
    "kotaTotalDataframe.select(kotaTotalDataframe['Kota'],\n",
    "                          kotaTotalDataframe['Total']+10).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita dapat memfilter data berdasarkan syarat kondisional tertentu seperti kode di bawah ini. Kode di bawah ini akan memembuat DataFrame baru dari baris data yang nilai di kolom \"Total\"nya lebih dari 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#memfilter data dimana nilai kolom 'Total' > 200\n",
    "kotaTotalDataframe.filter(kotaTotalDataframe['Total'] > 200).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk melakukan pengurutan/sorting data berdasarkan kolom tertentu, kita dapat melakukannya dengan fungsi \".orderBy('nama_kolom')\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orderByKotaDataframe = purchaseDataframe.orderBy('Kota').show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jika kita ingin mengetahui berapa jumlah transaksi di tiap kota, kita dapat menggunakan kode di bawah ini. Pertama kita grupkan dahulu berdasarkan kolom \"kota\", kemudian kita hitung dengan fungsi \".count()\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jumlahPembelianByKota = purchaseDataframe.groupBy(\"Kota\").count()\n",
    "jumlahPembelianByKota.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cara akses data dari DataFrame\n",
    "DataFrame Spark adalah data yang terdistribusi di klaster, sehingga kita tidak dapat mengakses komponen dataframe dengan indeks (baris,kolom) seperti layaknya kita dapat lakukan di DataFrame pada pandas. Untuk mengakses data berdasarkan baris, kita dapat mengakalinya dengan cara menambahkan satu kolom berupa \"incremental ID\". Kemudian kita dapat memilih baris data yang kita inginkan dengan menggunakan fungsi \".filter()\". Berikut ini contohnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#mengimport fungsi monotonically_increasing_id\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "purchaseTambahKolomIdDataframe = purchaseDataframe.withColumn(\n",
    "    \"indeks\", monotonically_increasing_id())\n",
    "purchaseTambahKolomIdDataframe.show(4)\n",
    "baris2Sampai4 = purchaseTambahKolomIdDataframe.filter((purchaseTambahKolomIdDataframe['indeks']<=4) & \n",
    "                                                      (purchaseTambahKolomIdDataframe['indeks']>=2))\n",
    "baris2Sampai4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kemudian jika ingin mengakses nilai berdasakan kolom, mudah saja, yakni dengan menggunakan fungsi \".select()\" seperti yang kita telah lakukan sebelumnya. Berikut contohnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataBaris2KolomTotal = purchaseTambahKolomIdDataframe.filter(\n",
    "    purchaseTambahKolomIdDataframe['indeks']==2).select('Total')\n",
    "dataBaris2KolomTotal.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membuat DataFrame dari data yang kita isikan secara manual\n",
    "Kita dapat membuat DataFrame yang berisi data yang kita buat secara manual, tidak dari file. Untuk melakukannya, salah satunya, kita dapat mebuat data tipe \"Row\", kemudian kita akan gabungkan beberapa data \"Row\" menggunakan perintah \"sc.parallelize\". Hasil dari perintah tersebut adalah berupa RDD. Untuk mengubahnya ke DataFrame, kita dapat menggunakan perintah \".toDF()\". Berikut contoh programnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mengimport tipe data Row\n",
    "from pyspark.sql import Row\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "df = sc.parallelize([ \\\n",
    "     Row(nama='Rony', umur=27, tinggi=168), \\\n",
    "     Row(nama='Andy', umur=26, tinggi=165), \\\n",
    "     Row(nama='Syeril', umur=27, tinggi=168)]).toDF()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memanipulasi DataFrame menggunakan bahasa SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mungkin di antara kita ada yang sudah familiar menggunakan bahasa SQL. Kita dapat memperoses DataFrame menggunakan bahasa SQL dengan tetap mengembalikan nilainya dalam tipe data DataFrame. Untuk melakukan hal tersebut, pertama kita harus membuat SQL temporary view. Kode di bawah ini contoh memilih data dari kolom \"Total\" dari DataFrame kita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#membuat sql temporary view\n",
    "purchaseDataframe.createOrReplaceTempView(\"purchaseSql\")\n",
    "\n",
    "#memilih hanya kolom Total dan Pembayaran dari sql view kita\n",
    "TotalPembayaran = spark.sql(\"SELECT Total, Pembayaran FROM purchaseSql\")\n",
    "TotalPembayaran.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kemudian kode di bawah ini adalah kode untuk mengurutkan/sorting DataFrame kita berdasarkan kolom \"Kota\" dengan bahasa SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mengurutkan berdasarkan 'Kota' secara alfabetis\n",
    "orderByKota = spark.sql(\"SELECT * FROM purchaseSql ORDER BY Kota\")\n",
    "orderByKota.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terakhir, kita akan mencoba memfilter baris data yang nilai di kolom \"Total\"nya lebih dari 200, dan kita urutkan berdasarkan metode pembayaran (kolom \"Pembayaran\"). Berikut adalah kodenya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter nilai kolom Total>50 dan urutkan berdasarkan cara pembayaran\n",
    "contohFilter = spark.sql(\"SELECT * FROM purchaseSql WHERE Total>200 ORDER BY Pembayaran\")\n",
    "contohFilter.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
