{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membaca data dan membuat skema data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertama kita akan membaca data ke DataFrame dan membuat skema dari data kita, persis seperti yang telah kita lakukan sebelumnya. Kita akan membaca data flight dan data airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/C:/Users/User/Documents/python/dataset/raw-flight-data.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34188\\4191384321.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m ])\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m flights = spark.read.csv('dataset/raw-flight-data.csv', \n\u001b[0m\u001b[0;32m     23\u001b[0m                          schema=flightSchema, header=True)\n\u001b[0;32m     24\u001b[0m \u001b[0mflights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 535\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Path does not exist: file:/C:/Users/User/Documents/python/dataset/raw-flight-data.csv"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#membuat session, untuk mengakses semua fungsi spark dan DataFrame API\n",
    "appName = \"Pengenalan DataFrame Spark\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(appName) \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "flightSchema = StructType([\n",
    "  StructField(\"DayofMonth\", IntegerType(), False),\n",
    "  StructField(\"DayOfWeek\", IntegerType(), False),\n",
    "  StructField(\"Carrier\", StringType(), False),\n",
    "  StructField(\"OriginAirportID\", IntegerType(), False),\n",
    "  StructField(\"DestAirportID\", IntegerType(), False),\n",
    "  StructField(\"DepDelay\", IntegerType(), False),\n",
    "  StructField(\"ArrDelay\", IntegerType(), False),\n",
    "])\n",
    "\n",
    "flights = spark.read.csv('dataset/raw-flight-data.csv', \n",
    "                         schema=flightSchema, header=True)\n",
    "flights.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airportSchema = StructType([\n",
    "  StructField(\"airport_id\", IntegerType(), False),\n",
    "  StructField(\"city\", StringType(), False),\n",
    "  StructField(\"state\", StringType(), False),\n",
    "  StructField(\"name\", StringType(), False),\n",
    "])\n",
    "\n",
    "airports = spark.read.csv('dataset/airports.csv', header=True, \n",
    "                          schema=airportSchema)\n",
    "airports.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita dapat menggabungkan dua DataFrame (flights dan aiports) jika kita perlukan. Hal ini mirip dengan pemrograman SQL di relational database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsByOrigin = flights.join(airports,\n",
    "                               flights.OriginAirportID == \n",
    "                               airports.airport_id).groupBy(\"city\").count()\n",
    "flightsByOrigin.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menghadapi data duplikat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing pertama adalah mengecek apakah data kita ada yang terduplikat atau tidak. Jika ada data yang terduplikat, kita dapat menghapusnya dengan fungsi \"dropDuplicates()\" pada DataFrame kita. Kode di bawah ini adalah membuat DataFrame dimana baris data duplikat sudah kita hapus. Untuk menghitung berapa jumlah baris data duplikat, kita dapat menghitungnya dengan cara jumlah baris data asli dikurangi dengan jumlah baris setelah kita hapus duplikatnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jumlahBarisDataFlight = flights.count(); \n",
    "print(\"jumlah baris data flight: \", jumlahBarisDataFlight)\n",
    "flightHapusDuplikat = flights.dropDuplicates(); \n",
    "print(\"jumlah baris data flight tanpa duplikat: \", flightHapusDuplikat.count())\n",
    "dataDuplikat = jumlahBarisDataFlight - flightHapusDuplikat.count(); \n",
    "print(\"jumlah baris data duplikat: \", dataDuplikat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita juga dapat menspesifikasi pengecekan duplikatnya khusus pada kolom tertentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "sc = SparkContext.getOrCreate()\n",
    "df = sc.parallelize([ \\\n",
    "     Row(nama='Roni', umur=27, tinggi=168), \\\n",
    "     Row(nama='Roni', umur=5, tinggi=165), \\\n",
    "     Row(nama='Roni', umur=27, tinggi=168)]).toDF()\n",
    "df.show()\n",
    "df.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mungkin akan terjadi error saat menjalankan \"sc.parallelize\" di atas, kita dapat mengatasinya dengan cara klik \"Kernel>Restart\". Menghapus duplikat berdasarkan kolom tertentu dapat kita gunakan misalkan kita tidak ingin ada duplikat data dengan ID yang sama. Kode di bawah ini adalah contoh pengecekan duplikat data berdasarkan kolom \"nama\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropDuplicates(['nama']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menghadapi missing value (data yang tidak lengkap/ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bersamaan dengan mendeteksi jika ada data yang terduplikat, kita juga dapat mendeteksi jika ada missing value. Kita dapat menghapus baris data yang terdapat missing value tersebut, atau mengisi missing value tersebut dengan nilai lain, misalnya nilai rata-rata dari kolom tersebut. Fungsi \"dorpna\" di bawah ini berguna untuk membuat DataFrame baru dimana baris data dengan missing value sudah terhapus. Kita dapat menentukan subset dari kolomnya yang akan kita cek missing value atau tidak, juga kita dapat menentukan apakah menghapus baris datanya ketika semua kolom tersebut missing value atau setidaknya ada satu missing value saja. Kode di bawah ini akan menghapus baris data jika setidaknya ada salah satu missing value di kolom \"ArrDelay\" dan \"DepDelay\". Kemudian kita akan menghitung berapa jumlah data yang terdapat missing value dengan cara mengurangi jumlah baris di data awal dikurangi dengan jumlah baris setelah data kita hapus saat terdapat missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsNoMissingValue = flights.dropDuplicates().dropna(\n",
    "    how=\"any\", subset=[\"ArrDelay\", \"DepDelay\"])\n",
    "jumlahBarisMissingValue = jumlahBarisDataFlight - flightsNoMissingValue.count()\n",
    "print(\"jumlah baris data missing value: \", jumlahBarisMissingValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menghapus data yang kosong terkadang merupakan pilihan yang kurang baik. Kita dapat menggunakan pilihan lain, mengisi data yang kosong dengan nilai reratanya misalnya. Berikut ini adalah kodenya. Adapun jumlah datanya adalah sama dengan jumlah baris data saat kita melakukan penghapusan data duplikat yang telah kita lakukan di atas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanArrDelay = flights.groupBy().avg(\"ArrDelay\").take(1)[0][0]\n",
    "print(\"mean ArrDelay: \", meanArrDelay)\n",
    "meanDepDelay = flights.groupBy().avg(\"DepDelay\").take(1)[0][0]\n",
    "print(\"mean DepDelay: \", meanDepDelay)\n",
    "flightsCleanData=flights.dropDuplicates().fillna(\n",
    "    {'ArrDelay': meanArrDelay, 'DepDelay': meanDepDelay})\n",
    "print(\"jumlah baris clean data flight: \", flightsCleanData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk menghitung nilai mean (rerata) di atas, kodenya agak kompleks dan mungkin kurang 'friendly' bagi pendatang baru Spark. Adapun menurut penilaian penulis, API DataFrame dari Spark memang tidak sefleksibel dan sekaya API DataFrame dari pandas python. Untuk menghitung mean dari suatu kolom di DataFrame Spark, kita dapat melakukannya dengan kode \"DataFrame_kita.groupBy().avg(\"kolom_yang_ingin_kita_hitung\")\". Akan tetapi dari kode ini kita mendapatkan data berupa DataFrame, sedangkan yang kita inginkan adalah nilainya langsung dalam float. Kita dapat mengakses isi DataFrame tersebut dengan mengambil baris pertama dengan perintah \"take(1)\", dan nilainya dalam bentuk list matrik baris-kolom. Selanjutnya kita akses nilai meannya yaitu di baris 1 dan kolom 1 dengan kode \"[0][0]\" (indexing dimulai dari 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mengeksplorasi statistik data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita dapat mengeksplorasi statistik dari data kita seperti nilai mean, standard deviasi, maksimal dan minimal dari setiap kolom data kita dengan menggunakan kode berikut. Kita sudah pernah melakukan hal yang sama di pengenalan DataFrame Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsCleanData.describe('DepDelay','ArrDelay').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya kita dapat menghitung nilai korelasi antar variabel delay keberangkatan (Depelay) dan delay pesawat tiba (ArrDelay). Seperti yang kita telah diskusikan di teori data mining di awal buku ini, jika antara dua variable memiliki keterkaitan/pengaruh yang kuat, maka nilai korelasinya akan mendekati 1 (-1 untuk korelasi negatif), sedangkan jika tidak saling terkait, maka nilanya akan 0. Berikut ini adalah kodenya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korelasiDelay = flightsCleanData.corr('DepDelay', 'ArrDelay')\n",
    "print(\"korelasi antara delay keberangkatan dan delay pesawat tiba: \", \n",
    "      korelasiDelay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari nilai di atas, dapat kita simpulkan bahwa delay keberangkatan akan sangat berpengaruh terhadap delay pesawat tiba. Hal ini juga logis, bahwa ketika pesawat delay berangkat, maka kemungkinan besar pesawat akan delay saat tiba di bandara tujuan. Kita juga melihat bahwa saat mengeksplorasi data ini kita menggunakan DataFrame \"flightsCleanData\", yaitu data bersih yang sudah kita preprocessing sebelumya. Adapun saat melakukan teknik-teknik data mining nanti, kita juga akan menggunakan data yang sudah kita preprocessing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
